{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'WordEmbeddingsModel' from 'sparknlp.pretrained' (c:\\Users\\Ronit Kataria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sparknlp\\pretrained\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\Habib University\\Semester7\\Deep-Learning\\Sindhi-Poetry-Classification-using-Deep-Learning\\src\\embedding.ipynb Cell 1\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Habib%20University/Semester7/Deep-Learning/Sindhi-Poetry-Classification-using-Deep-Learning/src/embedding.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msparknlp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mannotator\u001b[39;00m \u001b[39mimport\u001b[39;00m DocumentAssembler, Tokenizer\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Habib%20University/Semester7/Deep-Learning/Sindhi-Poetry-Classification-using-Deep-Learning/src/embedding.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkSession\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Habib%20University/Semester7/Deep-Learning/Sindhi-Poetry-Classification-using-Deep-Learning/src/embedding.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msparknlp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpretrained\u001b[39;00m \u001b[39mimport\u001b[39;00m WordEmbeddingsModel\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Habib%20University/Semester7/Deep-Learning/Sindhi-Poetry-Classification-using-Deep-Learning/src/embedding.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Initialize Spark session\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Habib%20University/Semester7/Deep-Learning/Sindhi-Poetry-Classification-using-Deep-Learning/src/embedding.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mappName(\u001b[39m\"\u001b[39m\u001b[39mSindhiPoetryEmbeddings\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mgetOrCreate()\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'WordEmbeddingsModel' from 'sparknlp.pretrained' (c:\\Users\\Ronit Kataria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sparknlp\\pretrained\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import DocumentAssembler, Tokenizer\n",
    "from pyspark.sql import SparkSession\n",
    "from sparknlp.pretrained import WordEmbeddingsModel\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SindhiPoetryEmbeddings\").getOrCreate()\n",
    "\n",
    "# Create the DocumentAssembler and Tokenizer\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(\"document\") \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "# Use a pre-trained Sindhi embeddings model\n",
    "embeddings = WordEmbeddingsModel.pretrained(\"w2v_cc_300d\",\"sd\") \\\n",
    "    .setInputCols([\"document\", \"token\"]) \\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(stages=[documentAssembler, tokenizer, embeddings])\n",
    "\n",
    "# Define your couplet poetry in Sindhi\n",
    "couplet_poetry = [\n",
    "    [\"مون کي اسپارڪ اين ايل پي سان پيار آهي\"],\n",
    "    [\"هل منهنجي سڀني زبانين کي سنڌي سنڌت جواڙو\"]\n",
    "]\n",
    "\n",
    "# Create a DataFrame with the couplet poetry\n",
    "data = spark.createDataFrame(couplet_poetry, [\"text\"])\n",
    "\n",
    "# Apply the pipeline to the data\n",
    "result = pipeline.fit(data).transform(data)\n",
    "\n",
    "# Show the result\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'PretrainedPipeline' has no attribute 'listPretrainedPipelines'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Habib University\\Semester7\\Deep-Learning\\Sindhi-Poetry-Classification-using-Deep-Learning\\src\\embedding.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Habib%20University/Semester7/Deep-Learning/Sindhi-Poetry-Classification-using-Deep-Learning/src/embedding.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msparknlp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpretrained\u001b[39;00m \u001b[39mimport\u001b[39;00m PretrainedPipeline\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Habib%20University/Semester7/Deep-Learning/Sindhi-Poetry-Classification-using-Deep-Learning/src/embedding.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m PretrainedPipeline\u001b[39m.\u001b[39;49mlistPretrainedPipelines()\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'PretrainedPipeline' has no attribute 'listPretrainedPipelines'"
     ]
    }
   ],
   "source": [
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "PretrainedPipeline.listPretrainedPipelines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "AI4Bharat/samantar_mr_bert is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Ronit Kataria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:261\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 261\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[0;32m    262\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Ronit Kataria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/AI4Bharat/samantar_mr_bert/resolve/main/vocab.txt",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Ronit Kataria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    416\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 417\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    418\u001b[0m         path_or_repo_id,\n\u001b[0;32m    419\u001b[0m         filename,\n\u001b[0;32m    420\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    421\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[0;32m    422\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    423\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    424\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    425\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    426\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    427\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    428\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    429\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    430\u001b[0m     )\n\u001b[0;32m    432\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[1;32mc:\\Users\\Ronit Kataria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ronit Kataria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1195\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[0;32m   1196\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m   1197\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[0;32m   1198\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1199\u001b[0m         timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[0;32m   1200\u001b[0m     )\n\u001b[0;32m   1201\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[0;32m   1202\u001b[0m     \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ronit Kataria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ronit Kataria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1541\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[0;32m   1532\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[0;32m   1533\u001b[0m     method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mHEAD\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1534\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1539\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[0;32m   1540\u001b[0m )\n\u001b[1;32m-> 1541\u001b[0m hf_raise_for_status(r)\n\u001b[0;32m   1543\u001b[0m \u001b[39m# Return\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ronit Kataria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:293\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    285\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[0;32m    286\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Client Error.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    287\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m make sure you are authenticated.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    292\u001b[0m     )\n\u001b[1;32m--> 293\u001b[0m     \u001b[39mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[39melif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m400\u001b[39m:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-653a5071-66d0052f6b1cdeab61281810;e6cc5fe4-25a9-41b8-8f96-818686dfcb71)\n\nRepository Not Found for url: https://huggingface.co/AI4Bharat/samantar_mr_bert/resolve/main/vocab.txt.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32md:\\Habib University\\Semester7\\Deep-Learning\\Sindhi-Poetry-Classification-using-Deep-Learning\\src\\embedding.ipynb Cell 2\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Habib%20University/Semester7/Deep-Learning/Sindhi-Poetry-Classification-using-Deep-Learning/src/embedding.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Load the pre-trained model and tokenizer for Sindhi\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Habib%20University/Semester7/Deep-Learning/Sindhi-Poetry-Classification-using-Deep-Learning/src/embedding.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mAI4Bharat/samantar_mr_bert\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Habib%20University/Semester7/Deep-Learning/Sindhi-Poetry-Classification-using-Deep-Learning/src/embedding.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(model_name)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Habib%20University/Semester7/Deep-Learning/Sindhi-Poetry-Classification-using-Deep-Learning/src/embedding.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model \u001b[39m=\u001b[39m BertModel\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Habib%20University/Semester7/Deep-Learning/Sindhi-Poetry-Classification-using-Deep-Learning/src/embedding.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Define your couplet poetry in Sindhi\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ronit Kataria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1800\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1798\u001b[0m             resolved_vocab_files[file_id] \u001b[39m=\u001b[39m download_url(file_path, proxies\u001b[39m=\u001b[39mproxies)\n\u001b[0;32m   1799\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1800\u001b[0m         resolved_vocab_files[file_id] \u001b[39m=\u001b[39m cached_file(\n\u001b[0;32m   1801\u001b[0m             pretrained_model_name_or_path,\n\u001b[0;32m   1802\u001b[0m             file_path,\n\u001b[0;32m   1803\u001b[0m             cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m   1804\u001b[0m             force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m   1805\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1806\u001b[0m             resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m   1807\u001b[0m             local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m   1808\u001b[0m             use_auth_token\u001b[39m=\u001b[39;49mtoken,\n\u001b[0;32m   1809\u001b[0m             user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m   1810\u001b[0m             revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m   1811\u001b[0m             subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[0;32m   1812\u001b[0m             _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1813\u001b[0m             _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1814\u001b[0m             _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[0;32m   1815\u001b[0m         )\n\u001b[0;32m   1816\u001b[0m         commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_vocab_files[file_id], commit_hash)\n\u001b[0;32m   1818\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(unresolved_files) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Ronit Kataria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\hub.py:433\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    417\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    418\u001b[0m         path_or_repo_id,\n\u001b[0;32m    419\u001b[0m         filename,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    429\u001b[0m         local_files_only\u001b[39m=\u001b[39mlocal_files_only,\n\u001b[0;32m    430\u001b[0m     )\n\u001b[0;32m    432\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m--> 433\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    434\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    435\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    436\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    437\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    438\u001b[0m     )\n\u001b[0;32m    439\u001b[0m \u001b[39mexcept\u001b[39;00m RevisionNotFoundError:\n\u001b[0;32m    440\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    441\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mrevision\u001b[39m}\u001b[39;00m\u001b[39m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    442\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfor this model name. Check the model page at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    443\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m for available revisions.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    444\u001b[0m     )\n",
      "\u001b[1;31mOSError\u001b[0m: AI4Bharat/samantar_mr_bert is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained model and tokenizer for Sindhi\n",
    "model_name = \"AI4Bharat/samantar_mr_bert\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Define your couplet poetry in Sindhi\n",
    "couplet_poetry = [\n",
    "    \"مون کي اسپارڪ اين ايل پي سان پيار آهي\",\n",
    "    \"هل منهنجي سڀني زبانين کي سنڌي سنڌت جواڙو\"\n",
    "]\n",
    "\n",
    "# Tokenize and convert to embeddings\n",
    "embeddings = []\n",
    "for couplet in couplet_poetry:\n",
    "    inputs = tokenizer(couplet, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "\n",
    "# 'embeddings' now contains the embeddings for each couplet\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'WordEmbeddingsModel' from 'sparknlp.pretrained' (c:\\Users\\Ronit Kataria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sparknlp\\pretrained\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\Habib University\\Semester7\\Deep-Learning\\Sindhi-Poetry-Classification-using-Deep-Learning\\src\\embedding.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Habib%20University/Semester7/Deep-Learning/Sindhi-Poetry-Classification-using-Deep-Learning/src/embedding.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msparknlp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpretrained\u001b[39;00m \u001b[39mimport\u001b[39;00m WordEmbeddingsModel\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Habib%20University/Semester7/Deep-Learning/Sindhi-Poetry-Classification-using-Deep-Learning/src/embedding.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m documentAssembler \u001b[39m=\u001b[39m DocumentAssembler() \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Habib%20University/Semester7/Deep-Learning/Sindhi-Poetry-Classification-using-Deep-Learning/src/embedding.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m.\u001b[39msetInputCol(\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Habib%20University/Semester7/Deep-Learning/Sindhi-Poetry-Classification-using-Deep-Learning/src/embedding.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m.\u001b[39msetOutputCol(\u001b[39m\"\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Habib%20University/Semester7/Deep-Learning/Sindhi-Poetry-Classification-using-Deep-Learning/src/embedding.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m Tokenizer() \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Habib%20University/Semester7/Deep-Learning/Sindhi-Poetry-Classification-using-Deep-Learning/src/embedding.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m.\u001b[39msetInputCols(\u001b[39m\"\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Habib%20University/Semester7/Deep-Learning/Sindhi-Poetry-Classification-using-Deep-Learning/src/embedding.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m.\u001b[39msetOutputCol(\u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'WordEmbeddingsModel' from 'sparknlp.pretrained' (c:\\Users\\Ronit Kataria\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sparknlp\\pretrained\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from sparknlp.pretrained import WordEmbeddingsModel\n",
    "documentAssembler = DocumentAssembler() \\\n",
    ".setInputCol(\"text\") \\\n",
    ".setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    ".setInputCols(\"document\") \\\n",
    ".setOutputCol(\"token\")\n",
    "\n",
    "embeddings = WordEmbeddingsModel.pretrained(\"w2v_cc_300d\",\"sd\") \\\n",
    ".setInputCols([\"document\", \"token\"]) \\\n",
    ".setOutputCol(\"embeddings\")\n",
    "\n",
    "pipeline = Pipeline(stages=[documentAssembler, tokenizer, embeddings])\n",
    "\n",
    "data = spark.createDataFrame([[\"مون کي اسپارڪ اين ايل پي سان پيار آهي\"]]).toDF(\"text\")\n",
    "\n",
    "result = pipeline.fit(data).transform(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
